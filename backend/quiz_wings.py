import os
from transformers import GPTNeoForCausalLM, AutoTokenizer

# Disable symlink warning on Windows
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"

# Debugging - Step by Step

print("Starting the quiz generation script...")

try:
    # Load a more capable model like GPT-Neo or GPT-J
    print("Loading model and tokenizer...")
    model_name = "EleutherAI/gpt-neo-1.3B"  # You can use gpt-j-6B for better results if possible

    model = GPTNeoForCausalLM.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    print("Model and tokenizer loaded successfully.")

    # Setup the quiz generation function
    def generate_quiz(topic):
        print(f"Generating quiz for the topic: {topic}")
        
        prompt = f"Create a quiz about {topic}. Include 3 multiple-choice questions with 4 distinct answer options each. Make sure the questions are clear, varied, and related to {topic}."
        print(f"Prompt: {prompt}")
        
        inputs = tokenizer(prompt, return_tensors="pt", max_length=200, truncation=True)
        print("Inputs ready for model.")

        # Adjust these settings for a better variety in generation
        outputs = model.generate(
            **inputs,
            max_length=500,
            do_sample=True,     # Ensures randomness
            temperature=0.7,    # Controls creativity
            top_p=0.9,          # Nucleus sampling
            num_return_sequences=1
        )
        
        print("Output generated by the model.")
        quiz = tokenizer.decode(outputs[0], skip_special_tokens=True)
        print(f"Generated quiz text: {quiz}")
        
        return quiz

    # Run the quiz generator
    if __name__ == "__main__":
        topic = input("Enter the topic for your quiz: ")
        print(f"You entered: {topic}")
        
        quiz = generate_quiz(topic)
        print(f"Generated Quiz: \n{quiz}")

except Exception as e:
    print(f"An error occurred: {str(e)}")
